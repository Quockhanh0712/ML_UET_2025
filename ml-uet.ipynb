{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T14:35:16.815618Z",
     "iopub.status.busy": "2025-06-08T14:35:16.815323Z",
     "iopub.status.idle": "2025-06-08T14:35:16.821627Z",
     "shell.execute_reply": "2025-06-08T14:35:16.820820Z",
     "shell.execute_reply.started": "2025-06-08T14:35:16.815597Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Config\n",
    "GLOVE_PATH = \"/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\"\n",
    "IMDB_CSV_PATH = \"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n",
    "EMBEDDING_DIM = 200\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_LEN = 320\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "PATIENCE = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thông tin liên quan đến notebook và dataset \n",
    "\n",
    "link dataset : https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n",
    "link glove : https://www.kaggle.com/datasets/rtatman/glove-global-vectors-for-word-representation\n",
    "\n",
    "link notebook in kaggle : https://www.kaggle.com/code/tqkhanh05/ml-uet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T14:35:23.208151Z",
     "iopub.status.busy": "2025-06-08T14:35:23.207455Z",
     "iopub.status.idle": "2025-06-08T14:36:06.781235Z",
     "shell.execute_reply": "2025-06-08T14:36:06.780388Z",
     "shell.execute_reply.started": "2025-06-08T14:35:23.208129Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocab: 100%|██████████| 40000/40000 [00:26<00:00, 1511.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:16<00:00, 24556.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18813 vectors for vocab words.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(IMDB_CSV_PATH)\n",
    "df['label'] = df['sentiment'].map({'positive':1, 'negative':0})\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Chia train-val 80-20\n",
    "train_size = int(len(df)*0.8)\n",
    "train_df = df.iloc[:train_size]\n",
    "val_df = df.iloc[train_size:]\n",
    "\n",
    "# Build vocab\n",
    "freq = {}\n",
    "for text in tqdm(train_df['review'], desc=\"Building vocab\"):\n",
    "    for token in tokenizer(text):\n",
    "        freq[token] = freq.get(token, 0) + 1\n",
    "\n",
    "sorted_freq = sorted(freq.items(), key=lambda x: x[1], reverse=True)[:MAX_VOCAB_SIZE-2]\n",
    "vocab = {\"<pad>\":0, \"<unk>\":1}\n",
    "for i, (word, _) in enumerate(sorted_freq, 2):\n",
    "    vocab[word] = i\n",
    "\n",
    "# Encode text -> ids + padding\n",
    "def encode(text):\n",
    "    tokens = tokenizer(text)\n",
    "    ids = [vocab.get(t, vocab[\"<unk>\"]) for t in tokens]\n",
    "    if len(ids) > MAX_LEN:\n",
    "        ids = ids[:MAX_LEN]\n",
    "    else:\n",
    "        ids += [vocab[\"<pad>\"]] * (MAX_LEN - len(ids))\n",
    "    return ids\n",
    "\n",
    "# Dataset wrapper\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df['review'].values\n",
    "        self.labels = df['label'].values\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(encode(self.texts[idx]), dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "train_dataset = IMDbDataset(train_df)\n",
    "val_dataset = IMDbDataset(val_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained GloVe embeddings\n",
    "def load_glove_embeddings(path, vocab, embedding_dim=100):\n",
    "    print(\"Loading GloVe embeddings...\")\n",
    "    embeddings_index = {}\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in tqdm(f, total=400000):\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "\n",
    "    embedding_matrix = np.random.uniform(-0.25, 0.25, (len(vocab), embedding_dim))\n",
    "    embedding_matrix[vocab[\"<pad>\"]] = np.zeros(embedding_dim)\n",
    "    found = 0\n",
    "    for word, idx in vocab.items():\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[idx] = vec\n",
    "            found += 1\n",
    "    print(f\"Found {found} vectors for vocab words.\")\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "embedding_matrix = load_glove_embeddings(GLOVE_PATH, vocab, EMBEDDING_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T14:36:11.797191Z",
     "iopub.status.busy": "2025-06-08T14:36:11.796511Z",
     "iopub.status.idle": "2025-06-08T14:36:11.804705Z",
     "shell.execute_reply": "2025-06-08T14:36:11.804018Z",
     "shell.execute_reply.started": "2025-06-08T14:36:11.797167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Attention module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim*2, 1)\n",
    "    def forward(self, lstm_output):\n",
    "        attn_weights = torch.softmax(self.attn(lstm_output).squeeze(-1), dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), lstm_output).squeeze(1)\n",
    "        return context\n",
    "\n",
    "# Model: CNN + BiLSTM + Attention + GRU\n",
    "class CNN_BiLSTM_GRU_Attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_dim=128, output_dim=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=0)\n",
    "        self.conv = nn.Conv1d(embedding_dim, 128, kernel_size=5, padding=2)\n",
    "        self.bilstm = nn.LSTM(128, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.gru = nn.GRU(hidden_dim*2, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)                   \n",
    "        emb = emb.transpose(1, 2)                \n",
    "        conv_out = torch.relu(self.conv(emb))   \n",
    "        conv_out = conv_out.transpose(1, 2)     \n",
    "        lstm_out, _ = self.bilstm(conv_out)      \n",
    "        gru_out, _ = self.gru(lstm_out)          \n",
    "        attn_out = self.attention(gru_out)      \n",
    "        dropped = self.dropout(attn_out)\n",
    "        out = self.fc(dropped)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T14:36:15.061645Z",
     "iopub.status.busy": "2025-06-08T14:36:15.061372Z",
     "iopub.status.idle": "2025-06-08T14:43:08.384297Z",
     "shell.execute_reply": "2025-06-08T14:43:08.383639Z",
     "shell.execute_reply.started": "2025-06-08T14:36:15.061625Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.3425 Acc: 0.8442\n",
      " Val Loss: 0.2401 Acc: 0.9051 Precision: 0.8909 Recall: 0.9235 F1: 0.9069\n",
      " Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 0.1911 Acc: 0.9264\n",
      " Val Loss: 0.2280 Acc: 0.9076 Precision: 0.9179 Recall: 0.8955 F1: 0.9066\n",
      " Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 0.1257 Acc: 0.9550\n",
      " Val Loss: 0.2553 Acc: 0.9050 Precision: 0.8940 Recall: 0.9193 F1: 0.9065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.0681 Acc: 0.9773\n",
      " Val Loss: 0.3347 Acc: 0.8961 Precision: 0.8689 Recall: 0.9333 F1: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.0187 Acc: 0.9953\n",
      " Val Loss: 0.4312 Acc: 0.8968 Precision: 0.9097 Recall: 0.8814 F1: 0.8953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.0113 Acc: 0.9978\n",
      " Val Loss: 0.4879 Acc: 0.8954 Precision: 0.9020 Recall: 0.8876 F1: 0.8947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.0079 Acc: 0.9988\n",
      " Val Loss: 0.4929 Acc: 0.8961 Precision: 0.8966 Recall: 0.8957 F1: 0.8962\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "# Setup device, model, loss, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN_BiLSTM_GRU_Attention(len(vocab), EMBEDDING_DIM, embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1)\n",
    "\n",
    "# Train and eval functions\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for x, y in tqdm(loader, leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        preds = out.argmax(dim=1)\n",
    "        total_correct += (preds == y).sum().item()\n",
    "    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item() * y.size(0)\n",
    "            preds = out.argmax(dim=1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    return total_loss / len(loader.dataset), total_correct / len(loader.dataset), precision, recall, f1\n",
    "\n",
    "# Train loop with early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1 = eval_epoch(model, val_loader, criterion)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}\")\n",
    "    print(f\" Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} Precision: {val_prec:.4f} Recall: {val_rec:.4f} F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\" Model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T14:44:53.783580Z",
     "iopub.status.busy": "2025-06-08T14:44:53.783310Z",
     "iopub.status.idle": "2025-06-08T14:45:02.806190Z",
     "shell.execute_reply": "2025-06-08T14:45:02.805364Z",
     "shell.execute_reply.started": "2025-06-08T14:44:53.783559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Eval | Loss: 0.2280 Acc: 0.9076 Precision: 0.9179 Recall: 0.8955 F1: 0.9066\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load best model for final eval\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "val_loss, val_acc, val_prec, val_rec, val_f1 = eval_epoch(model, val_loader, criterion)\n",
    "print(f\"Final Eval | Loss: {val_loss:.4f} Acc: {val_acc:.4f} Precision: {val_prec:.4f} Recall: {val_rec:.4f} F1: {val_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1835,
     "sourceId": 3176,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
